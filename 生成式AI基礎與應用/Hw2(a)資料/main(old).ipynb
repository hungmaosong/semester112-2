{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "789xYH-v1y0n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class arithmeticDataset(Dataset):\n",
        "    def __init__(self, split=\"train\") -> None:\n",
        "        super().__init__()\n",
        "        # read the data from arithmetic.csv\n",
        "        self.data = pd.read_csv('drive/MyDrive/Colab Notebooks/arithmetic.csv')\n",
        "        # split the data into train, validation (usually 9:1)\n",
        "        if split == \"train\":\n",
        "            self.data = self.data.iloc[:int(0.9 * len(self.data))]\n",
        "        else:\n",
        "            self.data = self.data.iloc[int(0.9 * len(self.data)):]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_seq = str(self.data.iloc[index]['src'])  # Convert to string\n",
        "        output_seq = str(self.data.iloc[index]['tgt'])  # Convert to string\n",
        "        return input_seq, output_seq\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "metadata": {
        "id": "haic1xhm29-s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class arithmeticModel(torch.nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, input_size, hidden_size, num_layers, in_features, out_features) -> None:\n",
        "        super().__init__()\n",
        "        # Define the embedding layer\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "        # Define the LSTM layer\n",
        "        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # Define the feedforward neural network layer\n",
        "        self.ffn = torch.nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, input_logits):\n",
        "        # Input the data into the embedding layer\n",
        "        embedded_input = self.embedding(input_logits)\n",
        "        # Input the embeddings into LSTM layer\n",
        "        lstm_output, (hidden_state, cell_state) = self.lstm(embedded_input)\n",
        "        # Input the output of LSTM layer to ffn and get output logits\n",
        "        output_logits = self.ffn(lstm_output)\n",
        "\n",
        "        return output_logits"
      ],
      "metadata": {
        "id": "D-dkdLDA3JDK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class arithmeticTokenizer:\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        # define tokens in the dictionary (e.g. +,-,1,2,3,4,5,6...)\n",
        "        self.tokens = ['+', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9' , '(', ')', '*', '=', ' ']\n",
        "\n",
        "    def encode(self, seqs):\n",
        "        # Get the maximum length of the sequences\n",
        "        max_length = max(len(seq) for seq in seqs)\n",
        "        # Pad sequences to the maximum length\n",
        "        padded_seqs = [seq.ljust(max_length) for seq in seqs]\n",
        "        # transform the input sequence to a tensor of ids\n",
        "        encoded_seqs = []\n",
        "        for seq in padded_seqs:\n",
        "            encoded_seq = [self.tokens.index(token) for token in seq]\n",
        "            encoded_seqs.append(encoded_seq)\n",
        "        return torch.tensor(encoded_seqs)\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # transform the output ids to sequences\n",
        "        decoded_seqs = []\n",
        "        for id_seq in ids:\n",
        "            decoded_seq = [self.tokens[id] for id in id_seq]\n",
        "            decoded_seqs.append(decoded_seq)\n",
        "        return decoded_seqs\n"
      ],
      "metadata": {
        "id": "H-wfKTed3M4b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(sample):\n",
        "    # sample 是一个由数据批次组成的元组\n",
        "    src_seqs, tgt_seqs = zip(*sample)\n",
        "\n",
        "    # 使用 arithmeticTokenizer 对源序列和目标序列进行编码\n",
        "    tokenizer = arithmeticTokenizer()\n",
        "    src_seqs = tokenizer.encode(src_seqs)\n",
        "\n",
        "    # Pad target sequences to the same length as source sequences\n",
        "    max_length = max(len(seq) for seq in src_seqs)\n",
        "    tgt_seqs = [seq.ljust(max_length) for seq in tgt_seqs]\n",
        "    tgt_seqs = tokenizer.encode(tgt_seqs)\n",
        "\n",
        "    # 将编码后的序列转换为张量\n",
        "    src_seqs_tensor = torch.tensor(src_seqs)\n",
        "    tgt_seqs_tensor = torch.tensor(tgt_seqs)\n",
        "\n",
        "    return src_seqs_tensor, tgt_seqs_tensor\n"
      ],
      "metadata": {
        "id": "ot_waTdt3Qnm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Instantiate the arithmeticDataset for training set and validation set\n",
        "train_dataset = arithmeticDataset(split=\"train\")\n",
        "validate_dataset = arithmeticDataset(split=\"validate\")\n",
        "\n",
        "train_dl = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "validate_dl = DataLoader(\n",
        "    dataset=validate_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Define the parameters for the model\n",
        "num_embeddings = len(arithmeticTokenizer().tokens)\n",
        "embedding_dim = 50\n",
        "input_size = embedding_dim  # The size of input to the RNN layer is the same as the dimension of embeddings\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "in_features = hidden_size  # The number of input features to the feedforward neural network layer\n",
        "out_features = num_embeddings  # The number of output features from the feedforward neural network layer\n",
        "\n",
        "model = arithmeticModel(num_embeddings, embedding_dim, input_size, hidden_size, num_layers, in_features, out_features)# Create model.\n",
        "\n",
        "# transmit the model to GPU memory\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Create optimizer.\n",
        "optim = torch.optim.Adam(\n",
        "    params=model.parameters(),\n",
        "    lr=1e-4,\n",
        ")\n",
        "\n",
        "objtv = torch.nn.CrossEntropyLoss() # Create objective function.\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for cur_epoch in range(epochs):\n",
        "    # activate the batch normalization and dropout layers in the model\n",
        "    model.train()\n",
        "    # process bar\n",
        "    tqdm_train_dl = tqdm(\n",
        "        train_dl,\n",
        "        desc=f'Train: epoch {cur_epoch}'\n",
        "    )\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    for batch in tqdm_train_dl:\n",
        "        # Clean up gradient.\n",
        "        optim.zero_grad()\n",
        "        # Forward pass.\n",
        "        src_seqs, tgt_seqs = batch\n",
        "        src_seqs, tgt_seqs = src_seqs.to(device), tgt_seqs.to(device)\n",
        "        output_logits = model(src_seqs)\n",
        "\n",
        "        # Calculate loss.\n",
        "        loss = objtv(output_logits.transpose(1, 2), tgt_seqs)\n",
        "        # Backward pass.\n",
        "        loss.backward()\n",
        "        # Gradient descent. model optimization\n",
        "        optim.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predicted_labels = torch.argmax(output_logits, dim=2)\n",
        "        num_correct += (predicted_labels == tgt_seqs).sum().item()\n",
        "        num_samples += tgt_seqs.size(0) * tgt_seqs.size(1)  # Multiply by batch size and sequence length\n",
        "\n",
        "    train_accuracy = num_correct / num_samples\n",
        "    print(f'Train Accuracy: {train_accuracy * 100:.2f}%')\n",
        "\n",
        "    # Save checkpoints.\n",
        "    # deactivate the batch normalization and dropout layers in the model\n",
        "    model.eval()\n",
        "    tqdm_validate_dl = tqdm(\n",
        "        validate_dl,\n",
        "        desc=f'validation: epoch {cur_epoch}'\n",
        "    )\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    for batch in tqdm_validate_dl:\n",
        "        # Input the data batch into the model and obtain the predictions\n",
        "        src_seqs, tgt_seqs = batch\n",
        "        src_seqs, tgt_seqs = src_seqs.to(device), tgt_seqs.to(device)\n",
        "        output_logits = model(src_seqs)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predicted_labels = torch.argmax(output_logits, dim=2)\n",
        "        num_correct += (predicted_labels == tgt_seqs).sum().item()\n",
        "        num_samples += tgt_seqs.size(0) * tgt_seqs.size(1)  # Multiply by batch size and sequence length\n",
        "\n",
        "    validation_accuracy = num_correct / num_samples\n",
        "    print(f'Validation Accuracy: {validation_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3bs09PV3TCO",
        "outputId": "0ebf69cf-e116-4fe9-9a81-dd03955789f3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: epoch 0:   0%|          | 0/37020 [00:00<?, ?it/s]<ipython-input-5-4bdf4f55a5ea>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  src_seqs_tensor = torch.tensor(src_seqs)\n",
            "<ipython-input-5-4bdf4f55a5ea>:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  tgt_seqs_tensor = torch.tensor(tgt_seqs)\n",
            "Train: epoch 0: 100%|██████████| 37020/37020 [14:55<00:00, 41.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 77.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "validation: epoch 0: 100%|██████████| 4114/4114 [01:16<00:00, 53.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 75.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: epoch 1: 100%|██████████| 37020/37020 [14:54<00:00, 41.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 77.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "validation: epoch 1: 100%|██████████| 4114/4114 [01:13<00:00, 55.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 75.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: epoch 2: 100%|██████████| 37020/37020 [14:56<00:00, 41.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 77.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "validation: epoch 2: 100%|██████████| 4114/4114 [01:13<00:00, 56.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 74.66%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}