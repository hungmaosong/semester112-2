{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2V49SGussdG"
      },
      "source": [
        "# char-RNN-文本生成\n",
        "## 教學目標\n",
        "使用 RNN 弄出一個基本的生成文字模型，幫助初學者上手 RNN\n",
        "\n",
        "## 適用對象\n",
        "適用於已經學過 PyTorch 基本語法的人\n",
        "\n",
        "## 執行方法\n",
        "在 Jupyter notebook 中，選取想要執行的區塊後，使用以下其中一種方法執行\n",
        "\n",
        "- 上方工具列中，按下 Cell < Run Cells 執行\n",
        "- 使用快捷鍵 Shift + Enter 執行\n",
        "\n",
        "## 大綱\n",
        "- [載入資料](#載入資料)\n",
        "- [前處理](#前處理)\n",
        "- [建立字典](#建立字典)\n",
        "- [超參數](#超參數)\n",
        "- [資料分批](#資料分批)\n",
        "- [模型設計](#模型設計)\n",
        "- [訓練](#訓練)\n",
        "- [生成](#生成)\n",
        "\n",
        "## 檔案來源\n",
        "- [Kaggle HC 新聞資料集](https://www.kaggle.com/alvations/old-newspapers#old-newspaper.tsv)\n",
        "- 下載後請放到路徑 `專案資料夾/data/old-newspaper.tsv`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paK_oRr5tNl4",
        "outputId": "9b77b7d1-d48a-4952-d8bd-d5c3c7b53d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4996ee3d8d09>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencc"
      ],
      "metadata": {
        "id": "Gw8STzsduw_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVkWj8xYssdI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.utils.rnn\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import opencc\n",
        "\n",
        "data_path = './gdrive/MyDrive/ikm_lab/GAI'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd7qSt-0ssdI"
      },
      "source": [
        "# 載入資料\n",
        "- 請務必先[下載](https://www.kaggle.com/alvations/old-newspapers#old-newspaper.tsv)資料後將資料放置到 `data` 資料夾之下\n",
        "- `tsv` 檔案類似 `csv`，只是用 `\\t` 做分隔符號\n",
        "- 資料內容包含\n",
        "\n",
        "|欄位|意義|資料型態|\n",
        "|-|-|-|\n",
        "|`Language`|語系|文字（類別）|\n",
        "|`Source`|新聞來源|文字|\n",
        "|`Date`|時間|文字|\n",
        "|`Text`|文字內容|文字|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prwGlvaJssdJ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(os.path.join(data_path + '/old-newspaper.tsv'), sep='\\t')\n",
        "# 看一下前幾筆資料是什麼樣子\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jwe0n0ZssdJ"
      },
      "source": [
        "# 前處理\n",
        "- 訓練目標為生成繁體中文字\n",
        "    - 所以只考量繁體中文的資料\n",
        "    - 類別為 `Chinese (Traditional)`\n",
        "    - 共約 333735 筆\n",
        "- 資料長度不一\n",
        "    - 畫出長度分佈圖\n",
        "    - 計算長度四分位數、最小值、最大值\n",
        "    - 為了方便訓練，只考慮長度介於 60~200 的新聞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIO7XWTgssdK"
      },
      "outputs": [],
      "source": [
        "df[df['Language'] == 'Chinese (Traditional)'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8e0_N3BssdK"
      },
      "outputs": [],
      "source": [
        "# 只取前7000筆，因爲原資料量太大了，不方便演示\n",
        "df = df[df['Language'] == 'Chinese (Traditional)'].iloc[:7000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRtgkXucssdK"
      },
      "outputs": [],
      "source": [
        "# 簡單做一下統計\n",
        "df['len'] = df['Text'].apply(lambda x: len(str(x)))\n",
        "print(df['len'].describe())\n",
        "print(df[df['len'] <= 200].shape[0])\n",
        "print(df[df['len'] >= 60].shape[0])\n",
        "print(df[(df['len'] >= 60) & (df['len'] <= 200)].shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ4d_hwpssdK"
      },
      "outputs": [],
      "source": [
        "# 按照字串長度篩選一下資料集，可做可不做，看需求\n",
        "df = df[(df['len'] >= 60) & (df['len'] <= 200)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY2NI3XissdK"
      },
      "source": [
        "# 建立字典\n",
        "- 無法直接利用純文字進行計算\n",
        "- 將所有文字轉換成數字\n",
        "- 字典大小約為 `7000`\n",
        "- 特殊字\n",
        "    - '&lt;pad&gt;'\n",
        "        - 每個 batch 所包含的句子長度不同\n",
        "        - 將長度使用 '&lt;pad&gt;' 補成 batch 中最大值者\n",
        "    - '&lt;eos&gt;'\n",
        "        - 指定生成的結尾\n",
        "        - 沒有 '&lt;eos&gt;' 會不知道何時停止生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em6Hod-AssdL"
      },
      "outputs": [],
      "source": [
        "# 一個dict把中文字符轉化成id\n",
        "char_to_id = {}\n",
        "# 把id轉回中文字符\n",
        "id_to_char = {}\n",
        "\n",
        "# 有一些必須要用的special token先添加進來(一般用來做padding的token的id是0)\n",
        "char_to_id['<pad>'] = 0\n",
        "char_to_id['<eos>'] = 1\n",
        "id_to_char[0] = '<pad>'\n",
        "id_to_char[1] = '<eos>'\n",
        "\n",
        "# 把所有資料集中出現的token都記錄到dict中\n",
        "for char in set(df['Text'].str.cat()):\n",
        "    ch_id = len(char_to_id)\n",
        "    char_to_id[char] = ch_id\n",
        "    id_to_char[ch_id] = char\n",
        "\n",
        "vocab_size = len(char_to_id)\n",
        "print('字典大小: {}'.format(vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ6J6la1ssdL"
      },
      "outputs": [],
      "source": [
        "# 把資料集的所有資料都變成id\n",
        "df['char_id_list'] = df['Text'].apply(lambda text: [char_to_id[char] for char in list(text)] + [char_to_id['<eos>']])\n",
        "df[['Text', 'char_id_list']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q8LAMUBssdL"
      },
      "source": [
        "# 超參數\n",
        "\n",
        "|超參數|意義|數值|\n",
        "|-|-|-|\n",
        "|`batch_size`|單一 batch 的資料數|64|\n",
        "|`epochs`|總共要訓練幾個 epoch|10|\n",
        "|`embed_dim`|文字的 embedding 維度|50|\n",
        "|`hidden_dim`|LSTM 中每個時間的 hidden state 維度|50|\n",
        "|`lr`|Learning Rate|0.001|\n",
        "|`grad_clip`|為了避免 RNN 出現梯度爆炸問題，將梯度限制範圍|1|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgncDLJCssdL"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "epochs = 3\n",
        "embed_dim = 256\n",
        "hidden_dim = 256\n",
        "lr = 0.001\n",
        "grad_clip = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r15x449HssdM"
      },
      "source": [
        "# 資料分批\n",
        "- 使用 `torch.utils.data.Dataset` 建立資料產生的工具 `dataset`\n",
        "- 再使用 `torch.utils.data.DataLoader` 對資料集 `dataset` 隨機抽樣並作為一個 batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSVqB7USssdM"
      },
      "outputs": [],
      "source": [
        "# 這裏的dataset是文本生成的dataset，輸入和輸出的資料都是文章\n",
        "# 舉個例子，現在的狀況是：\n",
        "# input:  A B C D E F\n",
        "# output: B C D E F <eos>\n",
        "# 而對於加減法的任務：\n",
        "# input:  1 + 2 + 3 = 6\n",
        "# output: / / / / / 6 <eos>\n",
        "# /的部分都不用算loss，主要是預測=的後面，這裏的答案是6，所以output是6 <eos>\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # input:  A B C D E F\n",
        "        # output: B C D E F <eos>\n",
        "        x = self.sequences.iloc[index][:-1]\n",
        "        y = self.sequences.iloc[index][1:]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch_x = [torch.tensor(data[0]) for data in batch] # list[torch.tensor]\n",
        "    batch_y = [torch.tensor(data[1]) for data in batch] # list[torch.tensor]\n",
        "    batch_x_lens = torch.LongTensor([len(x) for x in batch_x])\n",
        "    batch_y_lens = torch.LongTensor([len(y) for y in batch_y])\n",
        "\n",
        "    # torch.tensor\n",
        "    # [[1968, 1891, 3580, ... , 0, 0, 0],\n",
        "    #  [1014, 2242, 2247, ... , 0, 0, 0],\n",
        "    #  [3032,  522, 1485, ... , 0, 0, 0]]\n",
        "    #                       padding↑\n",
        "    pad_batch_x = torch.nn.utils.rnn.pad_sequence(batch_x,\n",
        "                                                  batch_first=True, # shape=(batch_size, seq_len)\n",
        "                                                  padding_value=char_to_id['<pad>'])\n",
        "\n",
        "    pad_batch_y = torch.nn.utils.rnn.pad_sequence(batch_y,\n",
        "                                                  batch_first=True, # shape=(batch_size, seq_len)\n",
        "                                                  padding_value=char_to_id['<pad>'])\n",
        "\n",
        "    return pad_batch_x, pad_batch_y, batch_x_lens, batch_y_lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd4bWLP1ssdM"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset(df['char_id_list'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYp9q3e5ssdM"
      },
      "outputs": [],
      "source": [
        "data_loader = torch.utils.data.DataLoader(dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWuAvp-ZssdM"
      },
      "source": [
        "# 模型設計\n",
        "\n",
        "## 執行順序\n",
        "1. 將句子中的所有字轉換成 embedding\n",
        "2. 按照句子順序將 embedding 丟入 LSTM\n",
        "3. LSTM 的輸出再丟給 LSTM，可以接上更多層\n",
        "4. 最後的 LSTM 所有時間點的輸出丟進一層 Fully Connected\n",
        "5. 輸出結果所有維度中的最大者即為下一個字\n",
        "\n",
        "## 損失函數\n",
        "因為是類別預測，所以使用 Cross Entropy\n",
        "\n",
        "## 梯度更新\n",
        "使用 Adam 演算法進行梯度更新"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNhof8yVssdM"
      },
      "outputs": [],
      "source": [
        "class CharRNN(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super(CharRNN, self).__init__()\n",
        "\n",
        "        # Embedding層\n",
        "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size,\n",
        "                                            embedding_dim=embed_dim,\n",
        "                                            padding_idx=char_to_id['<pad>'])\n",
        "\n",
        "        # RNN層\n",
        "        self.rnn_layer1 = torch.nn.RNN(input_size=embed_dim,\n",
        "                                        hidden_size=hidden_dim,\n",
        "                                        batch_first=True)\n",
        "\n",
        "        self.rnn_layer2 = torch.nn.RNN(input_size=hidden_dim,\n",
        "                                        hidden_size=hidden_dim,\n",
        "                                        batch_first=True)\n",
        "\n",
        "        # output層\n",
        "        self.linear = torch.nn.Sequential(torch.nn.Linear(in_features=hidden_dim,\n",
        "                                                          out_features=hidden_dim),\n",
        "                                          torch.nn.ReLU(),\n",
        "                                          torch.nn.Linear(in_features=hidden_dim,\n",
        "                                                          out_features=vocab_size))\n",
        "\n",
        "    def forward(self, batch_x, batch_x_lens):\n",
        "        return self.encoder(batch_x, batch_x_lens)\n",
        "\n",
        "    def encoder(self, batch_x, batch_x_lens):\n",
        "        batch_x = self.embedding(batch_x)\n",
        "\n",
        "        # 假設有個tensor : tensor([[1, 2, 3, 4],\n",
        "        #                        [9, 0, 0, 0]])\n",
        "        # 輸出就是：PackedSequence(data=tensor([1, 9, 2, 3, 4]),\n",
        "        #                         batch_sizes=tensor([2, 1, 1, 1]),\n",
        "        #                         sorted_indices=None, unsorted_indices=None)\n",
        "        # torch.nn.utils.rnn.pack_padded_sequence 會把batch當中的句子從長到短排序，建立如上所示的資料結構\n",
        "        # 就像上一個例子一樣，RNN會先吃第一個batch內的第一個batch_size，看到這個地方的batch_size爲2，所以此時RNN會吃兩個token，輸出一個2Xhidden_dim的向量組\n",
        "        # 然後看第二個batch_size, 此時爲1，少了一個，說明其中一個序列到頭了，那就取上一個輸出向量的第一個，再生成一個1Xhidden_dim的向量\n",
        "        # [\n",
        "        # [1,2,3],                data = [1,4,6,2,5,3]   output  data = [1p,4p,6p,2p,5p,3p]             [1p,2p,3p]\n",
        "        # [4,5,0], => pack_padded_sequence => batch_sizes = [3,2,1] => RNN => batch_sizes = [3,2,1] => pad_packed_sequence => [4p,5p,0]\n",
        "        # [6,0,0]         _                                                     [6p,0,0]\n",
        "        # ]\n",
        "        batch_x = torch.nn.utils.rnn.pack_padded_sequence(batch_x,\n",
        "                                                          batch_x_lens,\n",
        "                                                          batch_first=True,\n",
        "                                                          enforce_sorted=False)\n",
        "\n",
        "        batch_x, _ = self.rnn_layer1(batch_x)\n",
        "        batch_x, _ = self.rnn_layer2(batch_x)\n",
        "\n",
        "        batch_x, _ = torch.nn.utils.rnn.pad_packed_sequence(batch_x,\n",
        "                                                            batch_first=True)\n",
        "\n",
        "        batch_x = self.linear(batch_x)\n",
        "\n",
        "        return batch_x\n",
        "\n",
        "    def generator(self, start_char, max_len=200):\n",
        "\n",
        "        char_list = [char_to_id[start_char]]\n",
        "\n",
        "        next_char = None\n",
        "\n",
        "        # 生成的長度沒達到max_len就一直生\n",
        "        while len(char_list) < max_len:\n",
        "            x = torch.LongTensor(char_list).unsqueeze(0)\n",
        "            x = self.embedding(x)\n",
        "            _, (ht, _) = self.rnn_layer1(x)\n",
        "            _, (ht, _) = self.rnn_layer2(ht)\n",
        "            y = self.linear(ht)\n",
        "\n",
        "            next_char = np.argmax(y.numpy())\n",
        "\n",
        "            # 如果看到新的token是<eos>就說明生成結束了，就停下\n",
        "            if next_char == char_to_id['<eos>']:\n",
        "                break\n",
        "\n",
        "            char_list.append(next_char)\n",
        "\n",
        "        return [id_to_char[ch_id] for ch_id in char_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7J_jVDussdN"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(2)\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "model = CharRNN(vocab_size,\n",
        "                embed_dim,\n",
        "                hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2XPQ9W4ssdN"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'], reduction='mean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXgU5hnpssdN"
      },
      "source": [
        "# 訓練\n",
        "1. 最外層的 `for` 迴圈控制 `epoch`\n",
        "    1. 內層的 `for` 迴圈透過 `data_loader` 取得 batch\n",
        "        1. 丟給 `model` 進行訓練\n",
        "        2. 預測結果 `batch_pred_y` 跟真正的答案 `batch_y` 進行 Cross Entropy 得到誤差 `loss`\n",
        "        3. 使用 `loss.backward` 自動計算梯度\n",
        "        4. 使用 `torch.nn.utils.clip_grad_value_` 將梯度限制在 `-grad_clip` &lt; &lt; `grad_clip` 之間\n",
        "        5. 使用 `optimizer.step()` 進行更新（back propagation）\n",
        "2. 每 `1000` 個 batch 就輸出一次當前的 loss 觀察是否有收斂的趨勢"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwAaDbGMssdN"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "i = 0\n",
        "for epoch in range(1, epochs+1):\n",
        "    process_bar = tqdm(data_loader, desc=f\"Training epoch {epoch}\")\n",
        "    for batch_x, batch_y, batch_x_lens, batch_y_lens in process_bar:\n",
        "\n",
        "        # 標準DL訓練幾板斧\n",
        "        optimizer.zero_grad()\n",
        "        batch_pred_y = model(batch_x.to(device), batch_x_lens)\n",
        "        batch_pred_y = batch_pred_y.view(-1, vocab_size)\n",
        "        batch_y = batch_y.view(-1).to(device)\n",
        "        loss = criterion(batch_pred_y, batch_y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        i+=1\n",
        "        if i%10==0:\n",
        "            process_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    # 麻煩各位同學加上 validation 的部分\n",
        "    # validation_process_bar = tqdm(...)\n",
        "    # for ... in validation_process_bar:\n",
        "    #     pred = model..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}