{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arithmeticDataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        # read the data from arithmetic.csv\n",
    "        # split the data into train, validation (usually 9:1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # you can write some processing code here\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arithmeticModel(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # self.embedding = ... Embedding layer definition\n",
    "        # self.rnn = ... RNN layer definition\n",
    "        # self.ffn = ... FNN layer definition\n",
    "    def forward(self, input_logits):\n",
    "        # Input the data into the embedding layer\n",
    "        # Input the embeddings into rnn layer\n",
    "        # Input the output of rnn layer to ffn and get output logits\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arithmeticTokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # define tokens in the dictionary (e.g. +,-,1,2,3,4,5,6...)\n",
    "    def encode(self, seqs):\n",
    "        # transform the input sequence to a tensor of ids\n",
    "        return #return the encoded tensor\n",
    "                \n",
    "    def decode(self, ids):\n",
    "        # transform the output ids to sequences\n",
    "        return # return sequences\n",
    "    \n",
    "    # You can write a __len__ function to obtain how many tokens are there in the dictionary\n",
    "    # def __len__(self):\n",
    "    #     return len(self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The collate function of DataLoader\n",
    "def collate_fn(sample):\n",
    "    # sample is a tuple of the data pieces in a batch\n",
    "    # encode the sequences and pack them to tensors\n",
    "    return # return data that can be directly input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    # Parameters here\n",
    ")\n",
    "validate_dl = DataLoader(\n",
    "    # Parameters here\n",
    ")\n",
    "\n",
    "\n",
    "model = arithmeticModel()# Create model.\n",
    "\n",
    "# transmit the model to GPU memory\n",
    "\n",
    "# Create optimizer.\n",
    "optim = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-4,\n",
    ")\n",
    "\n",
    "\n",
    "objtv = torch.nn.CrossEntropyLoss() # Create objective function.\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for cur_epoch in range(epochs):\n",
    "    # activate the batch normalization and dropout layers in the model\n",
    "    model.train()\n",
    "    # process bar\n",
    "    tqdm_train_dl = tqdm(\n",
    "        train_dl,\n",
    "        desc=f'Train: epoch {cur_epoch}'\n",
    "    )\n",
    "    for batch in tqdm_train_dl:\n",
    "        pass\n",
    "        # Clean up gradient.\n",
    "        # Forward pass.\n",
    "        # Input the data batch into the model\n",
    "        # Calculate loss.\n",
    "        # Backward pass.\n",
    "        # (optional) It's better to perform gradient clipping to avoid gradient vanishing and explosion\n",
    "        # Gradient descent. model optimization\n",
    "        \n",
    "    # Save checkpoints.\n",
    "    # deactivate the batch normalization and dropout layers in the model\n",
    "    model.eval()\n",
    "    tqdm_validate_dl = tqdm(\n",
    "        validate_dl,\n",
    "        desc=f'validation: epoch {cur_epoch}'\n",
    "    )\n",
    "    for batch in tqdm_validate_dl:\n",
    "        pass\n",
    "        # Input the data batch into the model and obtain the predictions\n",
    "        # compte evaluation score (exact match) between predictions and ground_truth \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3917",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
